# Data.gov Agent ðŸ•µï¸

[![CI](https://github.com/shai-mann/data-gov-agent/workflows/CI/badge.svg)](https://github.com/shai-mann/data-gov-agent/actions)

> *"Why Google when you can government data?"* â€” Someone, probably

An agentic microservice that turns natural language questions into actual answers using the treasure trove of data hiding in plain sight on data.gov. Built with Hono, Bun, LangGraph, and a healthy dose of multi-agent orchestration.

*If you're looking for the UI repository, it's right [here](https://github.com/shai-mann/data-gov-ui)*

## What Does This Thing Actually Do?

You ask it a question like "What's the average household income in California?" and instead of shrugging or hallucinating, it:

1. ðŸ” Searches through data.gov's 250,000+ datasets to find relevant data
2. ðŸ¤” Evaluates each dataset to see if it's actually useful (spoiler: most aren't)
3. ðŸ’¾ Downloads the CSV, loads it into DuckDB, and runs SQL queries against it
4. ðŸ“Š Returns you an actual answer with citations and links to the source data

No hallucinations. No making stuff up. Just good old-fashioned data extraction with a modern twist.

## The Architecture (Or: How Many Agents Does It Take?)

This project uses a **five-agent orchestration system** because apparently one agent wasn't dramatic enough:

### ðŸŽ¯ Core Agent
The director. Coordinates the entire workflow from your question to the final answer. Clarifies your query, delegates to other agents, and formats the final response.

**Route:** `POST /research`

### ðŸ”Ž Search Agent
The researcher. Generates 5-7 clever search queries (with government-specific keywords like `maintainer:*census*`), searches data.gov, and coordinates parallel evaluation of up to 50 datasets before admitting defeat.

**Key feature:** Won't waste time searching for "climate 2023 data" when it can just search "climate data" and actually find useful results.

### ðŸ“‹ Eval Agent
The bouncer. Looks at each dataset's metadata and decides if it's worth investigating. Filters out garbage datasets before we waste compute downloading them.

**Valid formats:** CSV only. We're not dealing with your PDFs.

### ðŸ”¬ Resource Eval Agent
The deep diver. Does a two-stage evaluation:
1. **Shallow:** "Is this worth my time?" (checks metadata)
2. **Deep:** Downloads preview, analyzes columns, types, and sample values

Handles CSVs, DOIs, HTML pages, and even Excel files (because government agencies love their `.xlsx`).

### âš—ï¸ Query Agent
The data scientist. Loads the CSV into an in-memory DuckDB instance, generates SQL queries, executes them, evaluates the results, and iterates up to 10 times to get the right answer.

**Flow:** Plan â†’ Execute â†’ Evaluate â†’ Repeat (or give up gracefully)

## The Tech Stack

- **Runtime:** [Bun](https://bun.sh) â€” because life's too short for `npm install`
- **Framework:** [Hono](https://hono.dev) â€” lightweight, fast, and doesn't make you fight CORS for 3 hours
- **Agents:** [LangGraph](https://langchain-ai.github.io/langgraph/) â€” state machines meet AI agents
- **LLM:** OpenAI GPT-4o-mini â€” fast, cheap, and structured
- **Database:** [DuckDB](https://duckdb.org) â€” SQL for CSVs without the drama of a real database
- **Data Source:** [data.gov CKAN API](https://www.data.gov/developers/apis) â€” 250k+ government datasets

## Getting Started

### Prerequisites

- [Bun](https://bun.sh) installed
- OpenAI API key (set as `OPENAI_API_KEY`)
- Data.gov API key (set as `DATA_GOV_API_KEY`) â€” [get one here](https://api.data.gov/signup/)

### Installation

```bash
# Install dependencies
bun install

# Copy environment template
cp .env.template .env

# Add your API keys to .env
# OPENAI_API_KEY=sk-...
# DATA_GOV_API_KEY=...
```

### Development

```bash
# Start with hot reload
bun run dev

# The server runs on port 3000 by default
# POST to http://localhost:3000/research with:
# { "query": "your question here" }
```

### Testing

```bash
# Run tests
bun run test

# Note: Use `bun run test` not `bun test` because the `nock`
# HTTP mocking library doesn't play nice with Bun's test runner
```

### Code Quality

```bash
# Lint
bun run lint

# Format
bun run format

# Type check
bun run type-check

# Do all the things
bun run lint && bun run format && bun run type-check
```

## API Endpoints

### `POST /research`
The main endpoint. Ask it a question, get an answer.

**Request:**
```json
{
  "query": "What are the top causes of death in the United States?"
}
```

**Response:**
```json
{
  "output": "Based on the CDC data...",
  "dataset": {
    "id": "...",
    "title": "...",
    "url": "..."
  }
}
```

### `GET /ws`
WebSocket endpoint for real-time progress updates. Connect to see what the agents are doing behind the scenes.

**Connection:**
```javascript
const ws = new WebSocket('ws://localhost:3000/ws');
ws.onmessage = (event) => {
  const { type, data } = JSON.parse(event.data);
  console.log(`[${type}]`, data);
};
```

**Message types:**
- `state_transition` â€” Agent moving between states
- `sub_state_log` â€” Actions within a state
- `info` â€” General info
- `error` â€” Something went wrong

**Reconnection:** Pass `?connectionId=<id>` to reconnect to an existing session.

### `GET /health`
Returns `{ status: 'ok' }` if the server is alive. Riveting stuff.

### Testing Endpoints

- `POST /test/search` â€” Test search agent directly
- `POST /test/evaluate/:packageId` â€” Test eval agent on specific dataset
- `POST /test/query/:packageId` â€” Test query agent on specific dataset

## The Five Tools

Each agent has access to specific tools:

### 1. `packageSearch`
Searches data.gov CKAN API for datasets. Returns top 10 results with metadata (but strips resources/extras to save context).

### 2. `packageShow`
Gets full metadata for a specific dataset by ID. Includes resources, extras, and all the juicy details.

### 3. `datasetDownload`
Downloads CSVs and returns a preview. Has a caching layer (`workingDatasetMemory`) to avoid re-downloading the same dataset multiple times.

### 4. `doiView`
Extracts content from DOI links, HTML pages, and Excel files. Uses Cheerio for HTML parsing with smart semantic selectors.

### 5. `sqlQueryTool`
Executes SQL queries against the DuckDB in-memory database. Returns rows + column metadata.

## Project Structure

```
src/
â”œâ”€â”€ agents/                      # The five agents
â”‚   â”œâ”€â”€ core-agent/             # Orchestrator
â”‚   â”œâ”€â”€ search-agent/           # Dataset searcher
â”‚   â”œâ”€â”€ eval-agent/             # Dataset evaluator
â”‚   â”œâ”€â”€ resource-eval-agent/    # Resource evaluator
â”‚   â”œâ”€â”€ query-agent/            # SQL querier
â”‚   â””â”€â”€ index.ts                # Agent exports
â”œâ”€â”€ tools/                       # The five tools
â”‚   â”œâ”€â”€ packageSearch.ts        # Search data.gov
â”‚   â”œâ”€â”€ packageShow.ts          # Get dataset details
â”‚   â”œâ”€â”€ datasetDownload.ts      # Download CSVs
â”‚   â”œâ”€â”€ doiView.ts              # Extract DOI/HTML content
â”‚   â”œâ”€â”€ sqlQuery.ts             # Execute SQL
â”‚   â””â”€â”€ index.ts                # Tool exports
â”œâ”€â”€ lib/                         # Shared utilities
â”‚   â”œâ”€â”€ data-gov.ts             # CKAN API client
â”‚   â”œâ”€â”€ database.ts             # DuckDB instance
â”‚   â”œâ”€â”€ ws-logger.ts            # WebSocket logging
â”‚   â”œâ”€â”€ annotation.ts           # Zod schemas
â”‚   â”œâ”€â”€ data-gov.schemas.ts     # API response schemas
â”‚   â””â”€â”€ utils.ts                # Helper functions
â”œâ”€â”€ llms/                        # LLM config
â”‚   â”œâ”€â”€ openai.ts               # GPT-4o-mini setup
â”‚   â””â”€â”€ index.ts                # LLM exports
â””â”€â”€ index.ts                     # Hono server + routes
```

## How It Works (The Full Flow)

1. **User sends query** â†’ Core Agent
2. **Format Node** â†’ LLM clarifies and expands the query
3. **Search Agent** â†’ Generates 5-7 search queries and searches data.gov
4. **Eval Agent** (parallel fan-out) â†’ Evaluates each dataset's metadata
5. **Resource Eval Agent** (parallel fan-out) â†’ Deep evaluation of each resource
6. **Select Best Dataset** â†’ Chooses the most relevant dataset (or returns null)
7. **Query Agent** â†’ Downloads CSV, loads into DuckDB, runs SQL queries
8. **Evaluate Results** â†’ LLM analyzes results and suggests improvements
9. **Final Output** â†’ Core Agent formats response with full metadata

Throughout this process, WebSocket messages keep you updated on progress.

## Key Design Decisions (Or: Why We Did The Weird Things)

1. **DuckDB for SQL** â€” Enables SQL querying of CSVs without spinning up Postgres
2. **In-Memory Everything** â€” Fast, no persistence needed for research tasks
3. **LangGraph for Workflow** â€” Declarative graph-based agent orchestration beats spaghetti code
4. **Fan-out Parallelization** â€” Evaluate multiple datasets/resources concurrently
5. **Deferred Nodes** â€” Wait for all parallel branches before moving on
6. **WebSocket for UX** â€” Long-running operations need transparency
7. **Structured LLM Output** â€” Forces the LLM to return parseable JSON schemas
8. **CSV-First** â€” Primary resources must be CSV (DOI/HTML are context only)
9. **Plan-Execute-Evaluate** â€” Query agent separates planning, execution, and reflection
10. **Recursion Limits** â€” Search Agent maxes out at 50 queries, Query Agent at 10 SQL queries

## Environment Variables

```bash
OPENAI_API_KEY=sk-...           # OpenAI API key
DATA_GOV_API_KEY=...            # Data.gov API key (from api.data.gov/signup)
PORT=3000                        # Server port (optional)
```

## Deployment

This thing runs on [Vercel](https://vercel.com) with zero config. Just connect your repo and deploy.

```bash
# Or use the Vercel CLI
vercel
```

## Limitations (Because Nothing Is Perfect)

- **CSV Only:** Primary data sources must be CSV. PDFs, Word docs, and images need not apply.
- **Max 50 Search Queries:** Search Agent gives up after 50 searches if it can't find a relevant dataset.
- **Max 10 SQL Queries:** Query Agent stops after 10 attempts to avoid infinite loops.
- **No Streaming:** Responses come back all at once (but WebSocket provides progress updates).
- **English Only:** Because government data is almost exclusively in English.
- **Data.gov Only:** Not scraping other sources (yet).

## What Makes This Interview-Worthy?

This project demonstrates:

- âœ… **Multi-agent orchestration** with LangGraph's state machines
- âœ… **Parallel execution patterns** (fan-out/fan-in, deferred nodes)
- âœ… **LLM tool use** with structured output schemas
- âœ… **WebSocket real-time updates** for long-running operations
- âœ… **SQL query generation and execution** with DuckDB
- âœ… **Caching strategies** to avoid redundant API calls
- âœ… **Type safety** with TypeScript and Zod
- âœ… **Testing** with Vitest and HTTP mocking
- âœ… **Modern runtime** (Bun) and lightweight framework (Hono)
- âœ… **Production deployment** on Vercel

## Contributing

This is an interview project, so contributions aren't expected. But if you're reading this and want to suggest improvements, feel free to open an issue.

## License

MIT

---

Built with â˜• and a slightly unreasonable amount of agent orchestration.
